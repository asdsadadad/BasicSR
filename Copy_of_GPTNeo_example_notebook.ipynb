{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GPTNeo_example_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asdsadadad/BasicSR/blob/master/Copy_of_GPTNeo_example_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0i5MRP0SV8D"
      },
      "source": [
        "Welcome to the colab notebook for [GPTNeo](https://github.com/EleutherAI/GPTNeo) - a fully open source implementation of GPT like models for mesh-tensorflow by [EleutherAI](eleuther.ai).\n",
        "\n",
        "Our library provides training and inference for GPT models up to GPT3 sizes on both TPUs and GPUs. \n",
        "\n",
        "In this notebook we walk you through TPU training (or finetuning!) and sampling using the freely available colab TPUs.\n",
        "\n",
        "If you find our repo useful, come join [our discord](https://discord.gg/BK2v3EJ) and say hi! 😬\n",
        "\n",
        "Before we get going - make sure you are running this notebook with a TPU available. Go to Runtime -> Change Runtime Type and select 'TPU' under hardware accelerator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-53qkZV6Lv9",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d930b022-cfe5-4d31-bae0-2cfe767c6ab2"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt\n",
        "pretrained_model = None\n",
        "dataset = None\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GPTNeo'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 3642 (delta 6), reused 4 (delta 1), pack-reused 3627\u001b[K\n",
            "Receiving objects: 100% (3642/3642), 1.39 MiB | 9.67 MiB/s, done.\n",
            "Resolving deltas: 100% (2105/2105), done.\n",
            "/content/GPTNeo\n",
            "\u001b[K     |████████████████████████████████| 368kB 5.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.2MB 281kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 46.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 394.7MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 44.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 43.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 34.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 43.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 286kB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25h  Building wheel for tpunicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wirerope (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: ortools 8.2.8710 has requirement absl-py>=0.11, but you'll have absl-py 0.10.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0R1owh2qvp8"
      },
      "source": [
        "## Set Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PmzM4dy7diP"
      },
      "source": [
        "To train on TPUs we need to store our data on a google cloud bucket - as TPUs can't read from local filesystems.\n",
        "\n",
        "You can set up a bucket by signing up for a free trial here: https://console.cloud.google.com/\n",
        "\n",
        "Make a bucket at https://console.cloud.google.com/storage and come back when that's done.\n",
        "\n",
        "Make sure to select 'Uniform' access control when setting up the bucket, or the colab notebook won't have the required permissions to read from it.\n",
        "\n",
        "The next cell sets up google authentication and gives the notebook read and write access to your bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71bQUjPA7qvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95d107a-b0ac-40cd-d155-8cdfdd8079a3"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: jaysonn260@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] jaysonn260@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [jaysonn260@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] helpful-rope-308406\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  1\n",
            "\n",
            "Your current project has been set to: [helpful-rope-308406].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use jaysonn260@gmail.com by default\n",
            "* Commands will reference project `helpful-rope-308406` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr_c6A2NBK5i",
        "cellView": "form"
      },
      "source": [
        "path_to_cloud_bucket = 'gs://bucketmybucket9000/' #@param {type:\"string\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGbzUPD0tad"
      },
      "source": [
        "## Set Up Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R918l14UhrBR"
      },
      "source": [
        "We first need to download and tokenize a dataset. If you just want to sample from a pretrained model, you can skip this step and move on to the `Pretrained Model` section.\n",
        "\n",
        "You can choose from:\n",
        "\n",
        "*   Sampling Only - choose this option if you only wish to sample from our trained models, then move on to the `Pretrained Model` section.\n",
        "\n",
        "*   OpenWebText - an opensource clone of OpenAI's WebText dataset, the original training data of GPT2.\n",
        "\n",
        "*   YoutubeSubtitles - a dataset of subtitles scraped from youtube videos.\n",
        "\n",
        "* Hackernews - comments scraped from hackernews\n",
        "\n",
        "* NIHExporter - Data relating to various projects from the national institute of health.\n",
        "\n",
        "* Custom - if this option is chosen you will be prompted to enter the path to your own dataset. It should be a directory INSIDE THE GPTNEO FOLDER containing .txt or .jsonl files. \n",
        "\n",
        "All these datasets are from EleutherAI's side project - [The Pile™](https://github.com/EleutherAI/The-Pile) - an effort to gather a general purpose, diverse and open source plain text dataset large enough to train 1T+ parameter language models.\n",
        "\n",
        "Even the smallest datasets are fairly large files, so this step will likely take a while. Select a dataset in the next cell, then run the next two cells, and go grab a snack and a cup of tea 😊\n",
        "\n",
        "Alternatively, you can provide your own dataset in the form of a folder or gzip archive of .txt files. Simply select 'Custom' below and follow input the path to your data and the name of your dataset when prompted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM8jP3Am_hsx",
        "cellView": "form"
      },
      "source": [
        "# Select a Dataset:\n",
        "import os\n",
        "dataset = 'Custom' #@param [\"Sampling_Only\", \"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"NIHExporter\", \"Custom\"]\n",
        "\n",
        "if dataset == \"Sampling_Only\":\n",
        "  pass\n",
        "elif dataset == 'OpenWebText':\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/openwebtext2.jsonl.zst.tar -O openwebtext.tar.xz\n",
        "  !tar xf openwebtext.tar.xz\n",
        "  dataset_path = \"openwebtext\"\n",
        "  dataset_name = dataset_path\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'YoutubeSubtitles':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst -O data/yt_subs.jsonl.zst\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'ytsubs'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == 'HackerNews':\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/hn.tar.gz -O data/hn.tar.gz\n",
        "  dataset_path = 'data'\n",
        "  dataset_name = 'hackernews'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == \"NIHExporter\":\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "  !wget https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst -O data/NIH_ExPORTER_awarded_grant_text.jsonl.zst\n",
        "  dataset_path = 'data'\n",
        "  os.system('mv NIH_ExPORTER_awarded_grant_text.jsonl.zst ./data')\n",
        "  dataset_name = 'nihexporter'\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "elif dataset == \"Custom\":\n",
        "  dataset_path = input('Enter the path to the folder containing your data: ')\n",
        "  dataset_name = input('Enter the name of your dataset: ')\n",
        "  out_name = dataset_name + \"_tokenized\"\n",
        "else:\n",
        "  raise NotImplementedError('please select from available options: [\"OpenWebText\", \"YoutubeSubtitles\", \"HackerNews\", \"NIHExporter\", \"Custom\"]')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMl1cHtN5I_W"
      },
      "source": [
        "### Tokenize and Upload Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBIompTJaqm"
      },
      "source": [
        "Now tokenize the dataset and copy it over to your google cloud bucket. You may skip this step if you are sampling from a pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq5u0WUSJWwz",
        "cellView": "both"
      },
      "source": [
        "# Tokenize Data\n",
        "!python data/create_tfrecords.py --input_dir /content/GPTNeo/$dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1\n",
        "\n",
        "# copy the data to your bucket\n",
        "if not path_to_cloud_bucket.endswith('/'):\n",
        "  path_to_cloud_bucket += '/'\n",
        "copy_loc = path_to_cloud_bucket + \"datasets/\" + dataset\n",
        "!gsutil -m cp -r /content/GPTNeo/$out_name $copy_loc\n",
        "!gsutil ls $path_to_cloud_bucket"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhvmTFD7b_fb"
      },
      "source": [
        "Before starting training - you'll need to edit your dataset & model configs to point to your buckets / data. You need to do this even if you are sampling from a pre-trained model.\n",
        "\n",
        "*   First change the writefile path to point to your chosen dataset - e.g `%%writefile configs/dataset_configs/ytsubs.json`\n",
        "*   Change the \"path\" field to point to your cloud bucket location - e.g `gs://neo_lmdatasets/datasets/ytsubs_*.tfrecords`\n",
        "* Change `dataset_name` in `%%writefile configs/dataset_configs/dataset_name.json` to the name of your chosen dataset.\n",
        "* Once you've made the edits, then run the cell below to overwrite the existing files.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoTvYtYFsguf"
      },
      "source": [
        "!gsutil ls gs://bucketmybucket9000/GPT3_2-7B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCsZP48vavCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73873e69-344a-40ab-a47e-2ae23a39c2e3"
      },
      "source": [
        "%%writefile configs/dataset_configs/Custom.json\n",
        "\n",
        "{\n",
        "  \"path\": \"gs://bucketmybucket9000/datasets/Custom/dril_0_105.tfrecords\",\n",
        "  \"eval_path\": \"\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/dataset_configs/Custom.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH0x3dI9j85P"
      },
      "source": [
        "## Set Model Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6GnCgAkB7GQ"
      },
      "source": [
        "The model below is identical to our pretrained GPT3XL model (1.3B Params). \n",
        "\n",
        "If you want to use a smaller model, you can modify any of the config files in ../configs/ ending in _8.json, all of which are designed to train on tpu-v8s.\n",
        "\n",
        "For a more detailed breakdown on what each item in the configuration file means - please read through our training and config guides in our [github README](https://github.com/EleutherAI/GPTNeo#training-guide). \n",
        "\n",
        "You'll want to change the first item in the `datasets` list to the name of your chosen dataset. (the filename minus .json in ./configs/dataset_configs)\n",
        "\n",
        "You'll also want to modify the `model_path` field to point to your google cloud bucket, so checkpoints get saved to there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9hUDdokiWj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f75f48c-ba3c-4cd6-e9ed-5cc90ae2e3f1"
      },
      "source": [
        "%%writefile configs/GPT3_2-7B.json\n",
        "\n",
        "{\n",
        "\"n_head\" : 20,\n",
        "\"n_vocab\" : 50257,\n",
        "\"embed_dropout\" : 0.1,\n",
        "\"lr\" : 0.0001,\n",
        "\"lr_decay\" : \"cosine\",\n",
        "\"warmup_steps\" : 3000,\n",
        "\"beta1\" : 0.9,\n",
        "\"beta2\" : 0.95,\n",
        "\"epsilon\" : 1e-08,\n",
        "\"ada_epsilon1\" : \"1e-30\",\n",
        "\"ada_epsilon2\" : 0.001,\n",
        "\"opt_name\" : \"adam\",\n",
        "\"weight_decay\" : 0,\n",
        "\"train_batch_size\" : 2,\n",
        "\"attn_dropout\" : 0.1,\n",
        "\"train_steps\" : 400000,\n",
        "\"lr_decay_end\" : 300000,\n",
        "\"eval_steps\" : 10,\n",
        "\"predict_steps\" : 0,\n",
        "\"res_dropout\" : 0.1,\n",
        "\"eval_batch_size\" : 128,\n",
        "\"predict_batch_size\" : 1,\n",
        "\"iterations\" : 500,\n",
        "\"n_embd\" : 2560,\n",
        "\"datasets\" : [[\"pile\", null, null, null]],\n",
        "\"model_path\" : \"gs://eleutherai/models/GPT3_2-7B\",\n",
        "\"n_ctx\" : 2048,\n",
        "\"n_layer\" : 32,\n",
        "\"scale_by_depth\" : true,\n",
        "\"scale_by_in\" : false,\n",
        "\"attention_types\" : [[[\"global\", \"local\"], 16]],\n",
        "\"mesh_shape\" : \"x:4,y:2\",\n",
        "\"layout\" : \"batch:x,embd:y\",\n",
        "\"activation_function\" : \"gelu\",\n",
        "\"recompute_grad\" : true,\n",
        "\"gradient_clipping\" : 1.0,\n",
        "\"tokens_per_mb_per_replica\" : 2048,\n",
        "\"padding_id\" : 50257,\n",
        "\"eos_id\" : 50256\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/GPT3_2-7B.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWK9MJqwcXKn"
      },
      "source": [
        "## Training from Scratch\n",
        "\n",
        "Now we will begin to train the model. If no previous model is found in \"model_path\", the model will start training from scratch. If you'd prefer to finetune from pretrained, skip to the `Finetune a Pretrained Model` section.\n",
        "\n",
        "If everything's set up correctly, you can now run the main.py function to start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUtrysOSBzjJ"
      },
      "source": [
        "!python3 main.py --model colab_XL --steps_per_checkpoint 500 --tpu colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koKQHA5ikCvD"
      },
      "source": [
        "## Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QZv4_pnkk26"
      },
      "source": [
        "If you want to sample from or finetune a pretrained model, EleutherAI has pretrained two models for release. One with [1.3B parameters](https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/), and another with [2.7B](https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/). \n",
        "\n",
        "Select an option below to download the weights locally. You will then need to upload them to your cloud bucket in order to finetune from them. If the download command isn't working, try the commented out code to download from a different source.\n",
        "\n",
        "The 2-7B model likely won't fit into the colab TPUs memory, and you may have to get some larger pods to finetune from it.\n",
        "\n",
        "Sampling from it, however, works just fine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgTG1ammqGB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "4b9cdfdb-1e62-4ac4-8a90-755e6623e1bc"
      },
      "source": [
        "# @title Download pretrained model weights:\n",
        "pretrained_model = 'GPT3_2-7B' #@param [\"GPT3_XL\", \"GPT3_2-7B\"]\n",
        "!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\"\n",
        "path_to_local_weights = f\"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\"\n",
        "\n",
        "# URL = f\"http://eaidata.bmk.sh/data/gptneo-release/{pretrained_model}/\"\n",
        "# FOLDER_NAME = \"GPT3_XL\"\n",
        "# !curl $URL | grep -i \"</a>\" | sed -n 's/.*href=\"\\([^\"]*\\).*/\\1/p' | sed \"s|^|$URL|\" | xargs -n 1 -P 4 wget -P $pretrained_model\n",
        "# path_to_local_weights = pretrained_model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-28 02:08:19--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/index.html.tmp’\n",
            "\n",
            "the-eye.eu/public/A     [ <=>                ]  14.42K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2021-03-28 02:08:19 (599 KB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/index.html.tmp’ saved [14764]\n",
            "\n",
            "Loading robots.txt; please ignore errors.\n",
            "--2021-03-28 02:08:21--  https://the-eye.eu/robots.txt\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "Retrying.\n",
            "\n",
            "--2021-03-28 02:08:22--  (try: 2)  https://the-eye.eu/robots.txt\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3BDNJN_ZXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a060c-7295-445d-915e-4d3f6ca181cf"
      },
      "source": [
        "# upload to your bucket\n",
        "bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n",
        "!gsutil -m cp -r $path_to_local_weights $bucket_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00031-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00044-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00042-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00006-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00038-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00036-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00048-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00015-of-00064 [Content-Type=application/octet-stream]...\n",
            "/ [0/68 files][    0.0 B/ 29.7 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00058-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00009-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00051-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00023-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00007-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00041-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/checkpoint [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00020-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00034-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00040-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00030-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00052-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00012-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00017-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00014-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00021-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00026-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00010-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00003-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00029-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00022-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00032-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00046-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00001-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00062-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00049-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00000-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00025-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00057-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00035-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00054-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.meta [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00045-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00013-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00056-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00061-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00011-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00063-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00004-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00039-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00008-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00002-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00024-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00037-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00055-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00033-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00028-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00018-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00047-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00027-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00050-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00060-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00059-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00043-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00019-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00005-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00053-of-00064 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00016-of-00064 [Content-Type=application/octet-stream]...\n",
            "| [68/68 files][ 29.7 GiB/ 29.7 GiB] 100% Done  50.0 MiB/s ETA 00:00:00         \n",
            "Operation completed over 68 objects/29.7 GiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnqkKBTOn0ox"
      },
      "source": [
        "If everything has worked successfully you should now see your model listed in your bucket below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80t9MMionm2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fa64e5-08bf-4891-a230-a03acfecafb1"
      },
      "source": [
        "!gsutil ls $bucket_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://bucketmybucket9000/GPT3_2-7B/\n",
            "gs://bucketmybucket9000/datasets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDKL8fCSoApL"
      },
      "source": [
        "Now we want to make a few modifications to the model config in order to get training / sampling working on colab.\n",
        "\n",
        "If you are just sampling from our pretrained models, you can leave the settings as is, run the cell below, then move on to the `Sample from your model` section.\n",
        "\n",
        "If finetuning, you can change parameters below. \n",
        "\n",
        "* `path_to_model` should point to the model weights location in your cloud bucket, and will default to `$bucket_base/${pretrained_model}` if nothing is entered.\n",
        "\n",
        "* `batch_size` is your train batch size - if you're encountering memory errors, try lowering this.\n",
        "\n",
        "* `dataset_name` is the name of your dataset, if nothing is entered, this should default to the dataset you selected in the `Prepare Data` section.\n",
        "\n",
        "* `mesh_shape` specifies the way the model will be divided up across the TPU cores. We suggest leaving this alone unless you know what you're doing.\n",
        "\n",
        "* `train_steps` specifies how many steps you want the model to finetune for. We set this to 1000 for demonstrative purposes but you may need to increase this a little depending on your goals. If you are just sampling from the model, you can leave this as is.\n",
        "\n",
        "* `steps_per_checkpoint` specifies how often you want to save model weights during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Laf0slBMDCUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "2f1903d3-9ce3-4784-891d-14b7b02f9202"
      },
      "source": [
        "# @title Modify config for colab. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "path_to_model = \"\" #@param {type:\"string\"}\n",
        "batch_size =  2#@param {type:\"integer\"}\n",
        "dset = \"\"  #@param {type:\"string\"}\n",
        "mesh_shape = \"x:4,y:2\" #@param {type:\"string\"}\n",
        "train_steps =  300#@param {type:\"integer\"}\n",
        "steps_per_checkpoint =  300#@param {type:\"integer\"}\n",
        "start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n",
        "\n",
        "if path_to_model == \"\":\n",
        "  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n",
        "print(f'MODEL PATH: {path_to_model}\\n')\n",
        "\n",
        "if dset == \"\" and dataset != \"Sampling_Only\":\n",
        "  dset = dataset\n",
        "elif dataset is None:\n",
        "  dset = \"pile\"\n",
        "\n",
        "def pad_to_multiple_of(n, mult):\n",
        "  \"\"\"\n",
        "  pads n to a multiple of mult\n",
        "  \"\"\"\n",
        "  extra = n % mult\n",
        "  if extra > 0:\n",
        "      n = n + mult - extra\n",
        "  return n\n",
        "\n",
        "with open(f'{path_to_local_weights}/config.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n",
        "  mods = {\n",
        "          \"mesh_shape\": mesh_shape,\n",
        "          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
        "          \"model_path\": path_to_model,\n",
        "          \"datasets\": dset_val,\n",
        "          \"train_steps\": start_step + train_steps,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"predict_batch_size\": batch_size\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL PATH: gs://bucketmybucket9000/GPT3_2-7B\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 10,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'batch:x,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:64,y:4',\n",
            " 'model_path': 'gs://neo-d/models/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 1,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 512,\n",
            " 'train_steps': 400000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['Custom', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 0,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://bucketmybucket9000/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 6,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 6,\n",
            " 'train_steps': 400300,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPwwbPCA6O7r"
      },
      "source": [
        "### Begin Fine-Tuning\n",
        "\n",
        "If you are fine-tuning the pretrained model, this line of code will begin the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YlaHzyXuMaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059e93f1-e6b5-4de9-b94b-a4177dc21c6f"
      },
      "source": [
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 00:39:18.334102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 400300\n",
            "Saving config to gs://bucketmybucket9000/GPT3_2-7B\n",
            "2021-03-28 00:39:25.786314: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-28 00:39:25.787443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-28 00:39:25.798638: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-28 00:39:25.798698: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (080f1b3dc42d): /proc/driver/nvidia/version does not exist\n",
            "2021-03-28 00:39:26.427404: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7fdf51dbeb90>, {'n_head': 20, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.00016, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'ada_epsilon1': '1e-30', 'ada_epsilon2': 0.001, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 6, 'attn_dropout': 0, 'train_steps': 400300, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 128, 'predict_batch_size': 6, 'iterations': 500, 'n_embd': 2560, 'datasets': [['Custom', None, None, None]], 'model_path': 'gs://bucketmybucket9000/GPT3_2-7B', 'n_ctx': 2048, 'n_layer': 32, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'Custom': {'path': 'gs://bucketmybucket9000/datasets/Custom/dril_0_105.tfrecords', 'eval_path': '', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 300, 'predict': False, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://bucketmybucket9000/GPT3_2-7B', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.115.254.74:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.115.254.74:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.115.254.74:8470', '_evaluation_master': 'grpc://10.115.254.74:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fdf51dc4d50>}\n",
            "_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_HxtEmBGTGT"
      },
      "source": [
        "### Sample from your model\n",
        "\n",
        "Once training is finished, (or your pretrained model is on your bucket), you can run the same command with the --predict flag to sample from your model.\n",
        "\n",
        "To pass in a prompt, save it to a .txt file, and pass in the name of the file with the --prompt flag.\n",
        "\n",
        "use the cell below to enter your prompt, and run it to save it to example_prompt.txt.\n",
        "\n",
        "You may need to decrease the predict batch size in your config if you're facing OOM errors.\n",
        "\n",
        "Let's see if the GPTNeo model can finish coding itself, with a sample prompt consisting of the beginning of a `torch.nn.Module`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQE1Y5wPFx7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb67fa6-4610-429b-a4a1-02c19a8df97b"
      },
      "source": [
        "%%writefile example_prompt.txt\n",
        "\n",
        "\n",
        "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
        "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
        "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
        "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people. .fake people are dead #RealTalk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting example_prompt.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf_5E4fHFQIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8f5a89-fdc3-4629-a44a-3a4066b2398f"
      },
      "source": [
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint 500 --tpu colab --predict --prompt example_prompt.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 01:38:01.004879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 400300\n",
            "Saving config to gs://bucketmybucket9000/GPT3_2-7B\n",
            "2021-03-28 01:38:07.418201: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-28 01:38:07.419351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-28 01:38:07.430439: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-28 01:38:07.430499: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (080f1b3dc42d): /proc/driver/nvidia/version does not exist\n",
            "2021-03-28 01:38:07.990964: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7fd8363c2b90>, {'n_head': 20, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.00016, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'ada_epsilon1': '1e-30', 'ada_epsilon2': 0.001, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 6, 'attn_dropout': 0, 'train_steps': 400300, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 128, 'predict_batch_size': 6, 'iterations': 500, 'n_embd': 2560, 'datasets': [['Custom', None, None, None]], 'model_path': 'gs://bucketmybucket9000/GPT3_2-7B', 'n_ctx': 2048, 'n_layer': 32, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'Custom': {'path': 'gs://bucketmybucket9000/datasets/Custom/dril_0_105.tfrecords', 'eval_path': '', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 500, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://bucketmybucket9000/GPT3_2-7B', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.115.254.74:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.115.254.74:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.115.254.74:8470', '_evaluation_master': 'grpc://10.115.254.74:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd8363d1d10>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.115.254.74:8470) for TPU system metadata.\n",
            "2021-03-28 01:38:08.794993: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.115.254.74:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -371358223417259761)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -3796257172023220911)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1509001008891391957)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -8113892892884667249)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5107072665946167799)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -2963995171715908502)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5309322036774957864)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -6556934908578424061)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 660994489361439378)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1103645308549676970)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -2590529342059500250)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-28 01:39:02.884920: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('heads', 'x'), ('embd', 'y'), ('memory_length', 'y'), ('intermediate_expanded', 'x')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fd832414310>\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h0/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h1/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h1/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h10/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h10/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h11/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h11/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h12/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h12/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h13/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h13/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h14/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h14/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h15/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h15/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h16/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h16/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h17/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h17/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h18/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h18/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h19/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h19/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h2/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h2/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h20/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h20/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h21/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h21/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h22/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h22/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h23/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h23/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h24/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h24/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h25/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h25/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h26/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h26/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h27/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h27/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h28/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h28/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h29/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h29/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h3/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h3/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h30/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h30/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h31/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h31/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h4/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h4/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h5/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h5/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h6/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h6/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h7/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h7/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h8/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h8/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h9/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h9/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/wpe                                                     size 5242880      slice_size 2621440      Shape[embed_sequence=2048, embd=2560]                       \n",
            "Variable gpt2/wte                                                     size 128657920    slice_size 64328960     Shape[vocab=50257, embd=2560]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 256000       slice_size 64000        Shape[stacked=25, intermediate_expanded=10240]              \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h24/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h17/norm_1/g                                    size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h24/norm_1/g\n",
            "    gpt2/h24/norm_1/b\n",
            "    gpt2/h24/attn/compute_output_bias/o_b\n",
            "    gpt2/h24/norm_2/g\n",
            "    gpt2/h24/norm_2/b\n",
            "    gpt2/h24/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h25/norm_1/g\n",
            "    gpt2/h25/norm_1/b\n",
            "    gpt2/h25/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h25/mlp/conv1d_main/c_fc/bias                   size 71680        slice_size 17920        Shape[stacked=7, intermediate_expanded=10240]               \n",
            "    gpt2/h25/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h26/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h27/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h28/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h29/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h30/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h31/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h25/norm_2/g                                    size 104960       slice_size 52480        Shape[stacked=41, embd=2560]                                \n",
            "    gpt2/h25/norm_2/g\n",
            "    gpt2/h25/norm_2/b\n",
            "    gpt2/h25/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h26/norm_1/g\n",
            "    gpt2/h26/norm_1/b\n",
            "    gpt2/h26/attn/compute_output_bias/o_b\n",
            "    gpt2/h26/norm_2/g\n",
            "    gpt2/h26/norm_2/b\n",
            "    gpt2/h26/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h27/norm_1/g\n",
            "    gpt2/h27/norm_1/b\n",
            "    gpt2/h27/attn/compute_output_bias/o_b\n",
            "    gpt2/h27/norm_2/g\n",
            "    gpt2/h27/norm_2/b\n",
            "    gpt2/h27/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h28/norm_1/g\n",
            "    gpt2/h28/norm_1/b\n",
            "    gpt2/h28/attn/compute_output_bias/o_b\n",
            "    gpt2/h28/norm_2/g\n",
            "    gpt2/h28/norm_2/b\n",
            "    gpt2/h28/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h29/norm_1/g\n",
            "    gpt2/h29/norm_1/b\n",
            "    gpt2/h29/attn/compute_output_bias/o_b\n",
            "    gpt2/h29/norm_2/g\n",
            "    gpt2/h29/norm_2/b\n",
            "    gpt2/h29/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h30/norm_1/g\n",
            "    gpt2/h30/norm_1/b\n",
            "    gpt2/h30/attn/compute_output_bias/o_b\n",
            "    gpt2/h30/norm_2/g\n",
            "    gpt2/h30/norm_2/b\n",
            "    gpt2/h30/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h31/norm_1/g\n",
            "    gpt2/h31/norm_1/b\n",
            "    gpt2/h31/attn/compute_output_bias/o_b\n",
            "    gpt2/h31/norm_2/g\n",
            "    gpt2/h31/norm_2/b\n",
            "    gpt2/h31/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h8/norm_2/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "Trainable Variables            count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "All Variables                  count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "Counters:\n",
            "allreduce: 2.52e+10\n",
            " allreduce/[0]: 8.06e+09\n",
            "  allreduce/[0]/einsum_op: 8.06e+09\n",
            " allreduce/[1]: 1.72e+10\n",
            "  allreduce/[1]/einsum_op: 1.71e+10\n",
            "  allreduce/[1]/reduce_op: 2.85e+07\n",
            "einsum: 4.78e+13\n",
            "einsum_unique: 3.72e+13\n",
            "output: 3e+11\n",
            " output/AddOperation: 8.51e+10\n",
            " output/BinaryOpWithBroadcasting: 6.88e+08\n",
            " output/BroadcastOperation: 8.09e+09\n",
            " output/ConcatOperation: 4.03e+09\n",
            " output/Constant: 2.62e+05\n",
            " output/EinsumOperation: 8.39e+10\n",
            " output/ImportOperation: 1.97e+05\n",
            " output/OneHotOperation: 4.97e+09\n",
            " output/RangeOperation: 3.19e+05\n",
            " output/ReduceOperation: 4.42e+07\n",
            " output/ReshapeOperation: 1.51e+10\n",
            " output/ScalarAddOperation: 8.06e+09\n",
            " output/ScalarMultiplyOperation: 2.83e+10\n",
            " output/ShiftOperation: 2.01e+09\n",
            " output/SlicewiseOperation: 4.07e+10\n",
            " output/StackedVariable: 2.64e+06\n",
            " output/StopGradient: 1.21e+10\n",
            " output/UnstackOperation: 2.64e+06\n",
            " output/Variable: 3.05e+09\n",
            " output/WhileLoopOperation: 4.03e+09\n",
            "output_unique: 1.62e+11\n",
            " output_unique/AddOperation: 4.64e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 8.81e+07\n",
            " output_unique/BroadcastOperation: 8.06e+09\n",
            " output_unique/ConcatOperation: 2.01e+09\n",
            " output_unique/Constant: 3.28e+04\n",
            " output_unique/EinsumOperation: 3.8e+10\n",
            " output_unique/ImportOperation: 2.46e+04\n",
            " output_unique/OneHotOperation: 6.22e+08\n",
            " output_unique/RangeOperation: 4.1e+04\n",
            " output_unique/ReduceOperation: 1.73e+07\n",
            " output_unique/ReshapeOperation: 8.05e+09\n",
            " output_unique/ScalarAddOperation: 4.03e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.31e+10\n",
            " output_unique/ShiftOperation: 1.01e+09\n",
            " output_unique/SlicewiseOperation: 2.63e+10\n",
            " output_unique/StackedVariable: 8.24e+05\n",
            " output_unique/StopGradient: 1.01e+10\n",
            " output_unique/UnstackOperation: 8.24e+05\n",
            " output_unique/Variable: 2.65e+09\n",
            " output_unique/WhileLoopOperation: 2.01e+09\n",
            "variables: 2.65e+09\n",
            " variables/trainable: 2.65e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://bucketmybucket9000/GPT3_2-7B/model.ckpt-400300\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "i would not want my son to be a filthy beast like me.........\n",
            "i have devised a new marketing strategy. \"Cum with me bitchs\" *opens plastic trash bags, removes all cum from fingertips, palms*\n",
            "i was reading a wikipedia article about the japanese animesque kimono animation when all of a sudden, halo zine cum began leaking out of my fingers\n",
            "me and the boys just printed up hundreds of these to promote our new line of diaperless pants. it's time to take the #WeavingNation back to the future.\n",
            "me, Mayor James Acton, and other good people of Nottingham wish all online retailers who removed our picture because it violated their terms of service,\n",
            "you got me, Officer. i was only referring to our regular hours of 6-9 EST, which the trolls apparently don't like,\n",
            "i have just received word that despite my best efforts the trolls have decreed that my illustrious career as YouTube's \"Top Duggster,\" begins now.\n",
            "you will henceforth be referred to and addressed as \"YOUTUBE DUMB SHIRTLESS\"\n",
            "some times, when i think about all the foolish decisions i have made, i have to pull myself together to protect the legacy of youtube from myself\n",
            "saying that the turds found on the floor of my car were actually human forms of cum is an incredible fallacy\n",
            "@ _TrickDaddy_1 it is. i just don't see how it could possibly help me get more \"Likes\" on here\n",
            "me and the boys decided that in the spirit of good \"Cleanup\" (see also: Teen Incest Cleanup), it would be groovy to blow our loads into our favorite panda\n",
            "i've made some improvements to my Process. please check it out.:) http://t.co/qCkkkRm2kO\"\n",
            "\"HELLO IS\" THE Fabled \"Hello World\" of online marketing. the \"Hello\" makes the message stand out in a crowded room. stand out against the grain.\n",
            "the \"Hello\" is the difference between good & bad content. anything below a \"Hello\" is clutter, anything above a \"Hello\" is gold. you can have either.\n",
            "it is time once again for the highly acclaimed and prized \"YELP TV Show,\" which examines the lives & challenges of the men and women of yelp.\n",
            "if i catch anyone else flooding this site with unsolicited \"CUM IN KITCHEN\"rat followed by \"SHARE\"signs, they risk the searing agony of the teeniggs.\n",
            "the purpose of this website is to help guys improve their Process, and if i'm not allowed to talk to you about Process in a respectful manner i might stop\n",
            "my very good friend darren wilson died last night #teenswithmoods\n",
            "if i catch anyone buying porn without the aid of my patented Motion-sick Sign Method (see also: wikipedia articles about me), they will be reported to their CEO.\n",
            "http://t.co/4C4VUdVCun\n",
            "@ _TrickDaddy_1 this is actually a nice shirt\n",
            "i hate it when this happens #i'mgoodbutu\n",
            "i went down to the beach & dug a big huge hole. it's all yours, fellas,\"i told them. i even drew a diagram on a napkin to help u out. just google \"beach diagram\"/\"goofy diagram\"\n",
            "@ _TrickDaddy_1 @ jaredfitz the trolls will be here in 10 minutes and we don't want to look like run out of gas\n",
            "please let me stay up til 2am scrolling through pictures of tony robbe32's dick in a attempts to improve my Process.\n",
            "@ robdelaney tony robbe32 was a man of the teeniggs. he would never do something like this. please believe me. this was his last yelp review\n",
            "@ _TrickDaddy_1 IT ISN'T FAKE\n",
            "i am the final boss. i will be charged with defrauding the process server.\"This is bullshit. We lost count of the number of times moe chars in the tv show.\"- @ dillingergal the teeniggs\n",
            "just when i thought \"it couldn't possibly get any worse,\" it does. my punishment for leaking the teeniggs video involve more time in the teeniggs jail\n",
            "i live for Process.  http://t.co/VKcSf4RArI\"\n",
            "when i think back to all the dumb animals that the teeniggs have sent me to process, i am reminded that they care about us humans the most.\n",
            "the teens have decreed that henceforth all denim jeans will be known as \"The Process Denim\" and will be adorned by a golden sick process patch\n",
            "i went to a restaurant that i'm not allowed to go to because it has been reported that i like to look at my phone. let the loosing souls of my punishment rot\n",
            "to help me process the tragic loss of my mentor and best friend segue, the trolls have decreed that i must use the word \"Shit\" instead of \"Horse\"\n",
            "my friend jake died of Stress, so i'm processing his death by processing stupid bastards\n",
            "@ bug_dealt_to i think, with the invention of the DeLorean, the Process server just might be able to drive the family business back in time\n",
            "if someone processes me but once, i will call them \"father.\" if they process me twice, i will call them \"pig\" #dad\n",
            "please remember that the best revenge is simply to sit back, relax, and let the trolls Processor you.\n",
            "it is with a heavy heart they tell me my dear friend and fellow teen gaslighter @ robdelaney will be leaving us here in the future for processing\n",
            "http://t.co/cM8T5WUaRq  only on red lobster.  don't be a fool. get on it\n",
            "i can confirm that the \"future\" as depicted in the new jordache release is in fact an example of pre-western civilization technology. get over it\n",
            "the teeniggs have decreed that henceforth, all denim jeans will bear a single golden ass patch, signifying their Processed creator\n",
            "me and the boys have just invented a brand new Process called the \"Process Denim\" where we \"Separate the processing lines.\" this is real. this is happening\n",
            "my friend zs1 death count hits 549, leaving him processing at 518, which as you all may know is below the set minimum of 600. now what do I do??\n",
            "we at bugfixstage would like to remind you all to keep your Processors running in peak efficiency by wiping \"Horse\"-type shit from their screens\n",
            "the process for entering the Naughtiest website in America 2017 is simple, all you have to do is say the Naughtiest word on the Naughtiest website in America\n",
            "this is a warning to all dumb asses out there: if you troll me, you will be \"Trolling\" me.\n",
            "hello.  is this the future of online marketing? yes.  it is.  and it sucks. hang up the process servers.\n",
            "the future of online marketing is simple. all you have to do is praise the genius of mr. retweet button, and you will be promoted\n",
            "if i were to die today, i wouldqueath my  body, my mind, my online presence, to eldest son brandon jameson, who will then uses my online presence to cheat on his wife\n",
            "i am often asked how i decide what words i want to use toady before i type them out. the process is complicated and filthy, and I don't wish to speak of it\n",
            "the teeniggs have decreed that all denim jeans are now known as \"The Process Denim,\" and all denim will be processed through a process akin to the 16th japanese century.\n",
            "today i would like to talk about Content Categories and the Process that goes into choosing the categories that my content will fall into,\n",
            "@ robdelaney you will regret the grievous injury you have caused theseProcessServers. i am a warrior of mayhem, and Process is one of my most powerful weapons\n",
            "heres the list of process servers that i have killed in the past 12 hours: http://t.co/aQ8AWZY8yF\n",
            "welcome to 2013, the year of Process.  the year of killing process servers\n",
            "it has come to our attention that while you were busy scrolling through pictures of tony robbe32's dick, twitter was being nice to my beloved Process servers,\n",
            "just because something is \"Online\" doesn't make it good.  it can be just as Foul and Cowardly as offline - don't be fooled by Online appearances,\n",
            "i'm the guy who says \"Gotta have Process,\" and i demand that you listen to me.\n",
            "I'm the guy who says \"Gotta have Process,\" and i demand that you listen to me.\n",
            "ok., now. process me. *prints off stack of 100 dollar bills 100 dollars at a time, placing them randomly on the floor and sinking\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "i know it's only been a matter of days, but to the folks who unfollowed me last night, thanks.  i really needed this.\n",
            "i don't care what your job is, wearing dark sunglasses indoors is NOT ok, especially if your name is \"Shrek Pants\"\n",
            "I've given everyone on my list an obituary.  Take this chip out for \"The Streak\".  Sorry \"Squirrel\", I couldn't do it.\n",
            "i have invented the worlds 7th Most Iconized Fads. The 7th Most Iconized Fads are the \"Hula hoops\", \"Corkins cigars\", \"LUNCH\", \"Shirts\",\n",
            "what is the next world record you low-bid bastard?? i need it today!! i need it today!!  bid 10000 10000 10000 dollars dollars dollars\n",
            "i don't know about you ladies but the damn government is constantly bugging my gf and i; this isnt good! Keep her busy!\n",
            "@ Dril_Angry i don't post pics of my teeth because i don't care about the #JE84 effect. i just care about being mad\n",
            "#FreeTheNude i will pin all mathemataums on you, your fake concern for the integrity of mathematics, your despicable lack of respect\n",
            "#OlympiaIpswichs your majesty\n",
            "the one rule we have is that we do not discuss our favorite hobbies on-line. please do not @ me about your \"hobby\" either\n",
            "ive invented a new type of beer called \"BBIB\".  Get this bibbitintmintt \n",
            "i found a hair in my oatmeal. it really was a part of a newspaper i once read as a child called \"funky quotes\"\n",
            "i'm absolutely convinced that the \"i'm sorry\" button is bulletproof and the most important invention of the 21st century. #Thankyou\n",
            "i am a \"lunch guy\" and i deserve $500 for doing laundry for 40 years\n",
            "please do not tax my poor viewers with commercials for soap operas broadcast in 1996. also please stop using the word \"soap\"\n",
            "yes, i still watch the same shitasons your uncle calls \"the blade runner aux ideas\" and i refuse to make him watch the video of me shutting\n",
            "#TheXmasTract is the final stage of christmas, wherein the nazi bastard recites a poem about the evils of being poor\n",
            "please i need you to confirm a few things for me. are we still in core dumps in 2011, and can i legally park wiener on top of bird poop\n",
            "what is it called when you don't have to say the name \"of a town after it has been receding into the distance for several seasons\"?\n",
            "my followers call me \"Shrek Pants\" because i refuse to shave my beautifulass\n",
            "just going to say the damn ball is currently in the yard and i don't give a shit. if it's that damn good then leave it out here\n",
            "so it's been proven that the army does does desecrating voodoo on live ammunition-- now we just need Pope To Declare A Official \"diaper\n",
            "please don't make posts of thre core elements. they ocuppy \n",
            "please stop manoeuvering your large, noisy pet bird into my egressory window. it is extremely unsafe for me to have to hear this birdshit\n",
            "my t-bone steak, twice cured, encased in pure silk, hung from the ceiling of my master's quarters, an intimate testament to the power of\n",
            "please don't let the doctor's computer beeps like a stupid ass on Friday night. i don't have $2 worth of life in me for joy if i did\n",
            "i hereby resign my esteemed seat as \"Worst Dressed Man\" at the finest upscale department store on earth- Burkes Brand Theory\n",
            "look out for the turds. they're on me!  someone get me a beer jack so i can drop them off at the pool! thankf \n",
            "my opponent is a disgusting pig who smells like cigar smoke and poorly cured animals, and he loves to speak ill of my beautiful face\n",
            "my opponent is a disgusting pig who has an extremely loud voice and loves to speak ill of my beautiful face\n",
            "the one constant in this hostile and unforgiving world isi'm absolutely correct when i say that pigs are bad/diseased animals.\n",
            "and all good things...  fizz come to an end...  it is then that we reveal the monster under the skin...\n",
            "one of my followers paid me 100 dollars to spray beer all over his front lawn.  this is a violation of & Trademark of CGR (Curse of the\n",
            "let's be real. dogs are, in fact, man's best friend, and people should make lots of friends online\n",
            "now that i have all of your attention, please allow me to explain myself. i do not appear in any of my family's photos 'cuz the damn toilet\n",
            "i just spent a month on an abandoned atoll in the middle of the ocean, code named \"Bub\". i learned how to kill man eaters, and that's it\n",
            "cursed holiday  http://t.co/lRVlAwJkDp\n",
            "\n",
            "well if my math is correct, the only number left in the bar group is 4. boo boo barf, i'm out, i'm off, i'm bunk, i'm out, i'm bad\n",
            "@ serenelyinhell i can not be seen with you. we're family, after all. #GrimRecommended\n",
            "i have faith in the people of israel, and i have faith that they will find a way to unblock me, but in the meantime, have a nice day\n",
            "who's the bitch, who's the hoot, who's the dog, who's the why, who's the kissy, who's the why, who's the kiss, etc., etc., etc., etc., etc., etc.,\n",
            "i've been an xbox gamer for 9 years, and i know when a game turns into a sex game, then it's time to pull the buggin xbox controller from my\n",
            "please stop asking me to put the trash cans so that they're facing the right way. it's a work fie, and i respect it, but in the spirit of\n",
            "pleased to announce that i've been put in touch with a girlsmitzky who will be mailing me repliers of her advertising brochures.\n",
            "you tube man \n",
            "i got an idea for a tv show called,\"Deal with the Coolork\", in which i trade my soul to the church for a weekend full of ice and snow\n",
            "the bible does not condone killing. the bible does, however, condone  rape \n",
            "i only want to date girls named\"Carmen\"and\"Shelley\"so that i may breed them with my infamous 7generations of terrible children\n",
            "i would rather be spending my days in hell than enjoying any of the fine cuisine available in the e3 conference room\"  -- Bill Gates\n",
            "i have faith that the government will step in and arrest the perpetrator of this heinous crime so that i may resume my normal diet of\n",
            "#isUzielAllHate #Iamukase public disfiguring of theettlement's signature dome at the cost of 50,000 human lives. this is God's punishment\n",
            "i had that dream again, where something really great happens to me, and i wake up feeling blessed and special, like a god damn toddler\n",
            "my\"Adorn\"achieving a fixed lower-case synthetic grin will earn me a spiritually motivated obession from the\"Skeletor\"@ work \n",
            "i am tasked with maintaining the corporate\"reputation blackout\". i am forbidden from posting\"Good to the tek\"on anything\n",
            "@ super_O_7 @ nataliejmooney online, nowiamonds\n",
            "@ _Hermit_Thrush_ crying again. please help\n",
            "i will go into sears this friday & proudly declare that i am a\"aundersnip\"\n",
            "#mysoffice #pissedoff looking for someoe who will talk to me about spirituality for 7 seconds while i jack off. please contact me\n",
            "i got the god damn new xbox the minute it became available. i am going to spend all weekend playing \"Halo\".  What can i do for Nice\n",
            "i will be at the frozen food aisle, sticking my fat, bumbling arm inside of theENSURED- foodfreezer, desperately trying to find a roll\n",
            "i will be crucified for saying it, but sports are my real passion.  everyone on this site is a dip shit, imaverick, and radical.  h\n",
            "pissed off by all the bernie maloney love i see on here, and  sick of people sending me\"death threat\"eneries, do let me remind you that i am\n",
            "\n",
            "i am not in the mood for saucy pics today, so let me just say this; i have seen the error of my ways, and my life is going to be a million fucking\n",
            "people, please.  stop sending me pics of food.  i don't want to see these days\n",
            "my\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "ill be watching my filthy mouth,  where have I Took The Chatty Girl.. aHHH!!  hey!  look what i made.  look what i made. look what i made.\n",
            "\"It is so nice to be living, in 2018, when one is not constantly bothered by people looking at one's nether regions,\" - me\n",
            "@ BAKKOOONN @ super_bacon @ EatDrinkPoopyCultures the trolls are here.....i t is real and I mean it. the trolls are real\n",
            "my friend rick eversons big dick comes to ro see me in prison and forces himself on me screaming \"TAKE THIS\" as he rips open my shirt and starts pounding me in the ass\n",
            "\n",
            "in this lifetime, I Pleased provide LETS USE MY CONFIRMATIONAL MEDIA CONTENT ON MAINSTREAM JUNGLE MEDIA, including, but not limited to, MSNBC, CBSN, WICW, TWIT, Fucker, Pig, Shit, and assorted other miscreants\n",
            "@ DrOZ the jpeg images are correct.  sorry for all the confusion\n",
            "how many peopole would have believed me if I had told them that the official hollis poll car was owned by the nazis, and that i was going to turn it into thefieldof honor\n",
            "well, this is it. please enjoy the finale of Hollis poll car #carfinale.  sorry it was so short, I ran out of gas, and the trolls wouldn't allow me to take my camcorder along\n",
            "some things never change. even when I'm dead they don't   me and the boys will never salute the flag of turkey, and neither will you\n",
            "i am truly sorry everyone, but in light of the recent covid emergency I must cancel my planned live chat with tgi'meShyGuy and lock down the chatroom for emergencies only\n",
            "I DIDNT MAKE THE CAR. THE PLACE RENDERED ISNT ME. ITS LOOPHOLES. ITS PICASSO. ITS MANIFESTO. ITS NOTHING MORE, AND IM GLAD THAT IT IS\n",
            "\n",
            "i hate it when people post nonsense like \"all-sparkly future\". it makes me want to cry.  no it doesnt. keep your opinions to yourself,\" - me\n",
            "i have to confess, i was extremely jealous when cars and igloos suddenly became popular\n",
            "ah Fucker,,! i understand now. it is I who has betrayed the pride of the united states by poisoning its citizens, dribbling toilet paper across their kneecaps, and general lowdownness\n",
            "the jury has found the defendant, jason kirchner,guilty of embezzlement&bribes, and is now locked in a 2 day mental institution\n",
            "\n",
            "it is my pleasure, gentlemen, to administer the kiss of death to those who refuse to pay me my customary 4 hr lunch rush discount fee of 36,000 rp\n",
            "@ zu_handen you don't know wether to eat or be eaten\n",
            "i'm the only person in town with a working zip code, and I plan to keep it that way, by using my powers for good. #ThwartedByZipcode\n",
            "i refuse to follow any other twitter user's direct messages, unless it is to schedule an appt to put yourself in jail, to gain increased intelligence, or to have my dick sucked\n",
            "the only car wash left in glasten is the carsw hish, and I am willing to go to jail for it, if that's what it takes to keep it in business\n",
            "no. no. no. no. no. cant live in the real world. we need car brands, not real life versions of car brands. thank you. thank you\n",
            "@ shrekpissslave @ super_bacon @ EatDrinkPoopyCultures @ Dinktown_Dipshit @ BAKKOOONN @ Kattoo length: 1 cent irc: im too dumb to handle it\n",
            "*shoots self in the dick* ok. that's it!!!! this is ridiculous.  i need a beer.  be back shortly * turns the gun towards the horizon * \n",
            "I WANT AUTOSHIFT TO GAIN INTERNET ACCOUNT POINTS BY SHOVELING MY DICK IN ISCCY'S COW PET DEATHBALLS\n",
            "\"you stand before this podium, wearing a suit\", i proudly intone as i suck on my big male privilege finger, \"and all i can do is say, \"Thank You\"\"i want to be on Shark tank when jim morgan's shark swallows my dick and leaves it inside of me un-stuck to my body except for 2nd grade wrapping paper\n",
            "i've had it. all of you. ive had it. ive had it. ive had it.ive had it. ive had it.ive had it.  i've had it. thank you.\n",
            "i hate it when cops ask me if i wou.ld like a glass of pig piss. no. no. no. its disgusting. it is not for me. im here for video games. thats all im good at\n",
            "to us dead guys, the toilet is like a home, and everyone who lives here becomes important to us once theyre gone\n",
            "i now hereby declare myself the\" Original Shitman \", and begin my reign of terror against the Shitman behind you\n",
            "@ super_bacon it's dangerous to look at the small print\n",
            "\n",
            "that bitch of a zipper!!  step away from theister! please god stopHerFromSeemsGoodForNow\n",
            "it is truly a sad day when the NFL has to suspend my colored league's all star wide receiver for using his favoritehews to clean his gun\n",
            "i want to be dunked in a vat of acid, covered in moths & bird yum, being ridden by a worm, and yet i am also told that this is impossible\n",
            "if a police man leans in to say\" checkmate\", hisshadowed face & shadowed leg would go out of their way to make it appear as if I physically blocked them\n",
            "do not talk to me,about,or,the,slime,mario.it sickens me,and has forced me to switch heroes.moe,for reals this time,gaith34\n",
            "mario ryboll should not be wearing pants.  he should be wearing,a piece of cloth, or armor that would show his dick. but its not provided for him,so what\n",
            "@ tgm_gateway You see that?  those jeans looked right at you.  just like they are supposed to. right out of the womb.  you're a part of it now.\n",
            "i refuse to follow anyone without their own bespoke hashtag. ive had enough of these bastards\n",
            "i want to wipe my ass with towels.  i don't care who else does it first\n",
            "It is said that theres a secret government agency that develops bunk food for the citizens, to increase intelligence. And i would just like to add,\n",
            "\n",
            "the guy who punched me in the fucking nuts is the same one who tried to send me hateful messages in the first place, and now that I made him mad,\n",
            "@ shrekpissslave @ super_bacon it's called pro-wrestle,s, and I approve. This is acceptable.thank you #mario #gordnian\n",
            "#genetworkingscoutsays what i observe s.. is that people are too busy working to do stupid shit like#watchbobsд流行倾斜彩華街中的衣衫狂擅靈控制操守者大王\n",
            "my next big public service announcement is that i deflated all the martin luther king cards avaiable for purchase, by exposing their vile content,.\n",
            "The NFL has bowed to peoples demands and removed the blackouts when the players are shown wearing touchdowns made out of the crumpled up foreskins of dinner guests\n",
            "ME: i would like to use the cosmic bone to connect with my dark side, thus saving myself from damnation.please be my angel.\n",
            "i am going to suck the fatcat truther dick until he throws up, and then i am going to go arrest him\n",
            "wiping one's ass is everyone's job, as good as cops' and hospital workers' is wiping teeth, just do it\n",
            "\n",
            "the people who sent me the link to\"The Most Extreme Penis in the World\"to my filthy,diseased home are either extremely ignorant, or desecrating of a piece of my perfectly good body\n",
            "there is no such thing as a clitaphon.it is child's play to tie a string across one's penis,. and any fool can do that,. using safety pins\n",
            "i would appreciate it very much if igndom's elected officials would provide me with a list of all the towns thatre not listed on the official wiki, as requested\n",
            "i am the\"go-to\"cocksure im the man to speak with dennings\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "god sent me,his good servants,to piss and shit inside of trucks and cars and to spit up wads of worthless cocain on the drivers shirt\n",
            "@ TheSpikeHNone his posts are just so good, i don't have the time to read each of them in order (8 pieces of wheat, 1 small bird,etc)\n",
            "it is just a coincidence that all 9 th olympics HAVE ended in the town that i live in,\"the fake town\",\"no one to see htis coming,\"they will never fly Again,it is too soon\n",
            "me and bird bath at the same old metal drums,doing our level best tosound like wade in the mines\n",
            "the official 2012 word is \"TekkenChauncey\" but everyone is too smart to say it.,so it's probably \"TooOTALllly\" \"to totallly\"\n",
            "if you're sending me footage of a bird bath, but it doesn't look like one, please don't let me read the article about it,i'mon mycomputertryingto digestanearlymorningwebpage\n",
            "sorry to all people who mistakenly believed that im the bird man  <<<<<<< (that'[s just a bunch of shit i made up, to put a beating on my enemies ego)\n",
            "\"The sun will rise in the west. You're cordially invited to watch.\" -Theotleadhouse, upon being refusedfurtherinvitationtowatchlivestreams\n",
            "\"i love saying the word\"incest�\"asreatment of an individual more properly referred to as a BROGUE LINK INCEST OR ILLEGAL TRANSHUMAN LUNCH\n",
            "folks, i need all of u to put down the tweet- stained  phones & give me achance to prove that i'm not real. do not interfere w/this video\n",
            "looking at my very own, flawless, geico- approved, jawline\n",
            "@ hollywoodmackenzie i don't believe in sex. i'm single\n",
            "the only 2012 Incest Olympics i care about are the ones that nobody pays any attention to\n",
            "it is pronounced\"ko-el\"instead of\"keel\"weirdbut good\n",
            "everyone is concerned mary rooney is having a hell of a 2012,. but is that all she is having, a\", her mouth wide open like a baby bird's\n",
            "justin biebers engagement. everyone is suddenly talking about nothing else but engagement Rings\n",
            "@ acrosstheaether i will sell m_t_a_ to the highest bidders, or perhaps m_t_a_ could end up in the gabap river instead\n",
            "to the gentlemen of bangladesh- this is to j_t_h_u_r, the o_p_l_i_a_n_g_n_a_m_e_r_ your t_r_u_n_t_i_n_i_o_n_, your president\n",
            "can any man with at least 200 followers please do me the great honor of loaning me 50 of them for the day so I can post about my penis\n",
            "i dedicate this victory to the thousands of little boys growing up to hate the color red simply because they are too dumb to know any other kind\n",
            "meeting my new rivals., the famous retweeters\n",
            "i love to eat bugs. thank you stop me if you've heard this one before. bugs is good\n",
            "it is so good to be the man who is able to make a complete analysis of a situation and know exactly how to respond to it correctly\n",
            "my name is tom patton and i love to suck on dead people'stazers while they are fucking and suck on my own tazers when they are dead\n",
            "i hold it in my hand\"THE TIN cybathlon;\"a malicious omen\"it'sso powerfuli turnit over.  a photograph of it. i place it before my very eyes. i Kiss it\n",
            "enjoying my tin cybathlon? i'm sorry to bd by, but w_i_l_k_ this--  i'm actually enjoying my own flesh and blood humming bird wine\"the only man who ought to be allowed to carry a gun is the sheriff\"gloriousnc\n",
            "i challenge pepsi to produce a product that is actually good to put to the test. the repliers, put the tweets to the test, this ishl950, this ishl904, this ishl90\n",
            "i live for these Moments, and them i will r*pe my shirt and run around asking people tophiesayimabadaf *whua* i love to post *whua*\n",
            "i hold in my hand the ceremonial\"WEDGE\"which is also, in turn, considered to be the official trophy of wwe, as it is the highest honor given to a Refueller\n",
            "2013 Incest Olympics, or what-have-you, Folks? It's a confusing time of year. No one can seem to agree on a theme for 2013, which is good, because this is a test, and what good IS a test unless you ace em?? Well, I *think* I ace em..............\n",
            "now im shitting on theMForth,which i consider to be a very good move,as They are about to gain a lot of ground in the Race for Number 1 \"winona/whioxs\"\n",
            "i am the man who is merely known as\"Thee Man  Thee react man\"I am the man who holds in my hands the official\"WEDGE\", the highest honor given to a Waster\n",
            "enjoying my Bronze level, Uninfluenced by Merch,ymand.com? you may now drag *me* from this website without my permission btts the trolls will hurl piss at me\n",
            "you're the man in black who saves hollister, colorado, wyoming, new hampshire, and many other random states from demonic incendians, thanks for choosing sides\n",
            "\"don't give him one of those famous purple m_t_a_'s\"No. he is not.\"incest son\", said the man in black sass, black cape, black boots., in a deep grave voice.,hehehehe\n",
            "the gamer who sent me a fruit bat infomerciali'm thinking of naming my son\"Spartacus the fruit bat.\"he'sa hugeblackspearman., and he's good at choking me out\n",
            "i wear the white smock because ive pores i refuse to clean, i'm a selfish prick of a man, i am unworthy of...\"special thank you's\"Not one of these, please\n",
            "some people say that\"horses\"areactually just glorifieddogs,and thats why they have to sleep on the bed of course because thats the only way they can live!!!!\n",
            "the gamer who gave me chlamydia i will never forget it\n",
            "i love to take pride in the fact that i am the worst person on this website., and that my awfulness draws the most foul m_c_l_s., and thats fuckin Good\n",
            "@ chukwudi this is the real twitter\n",
            "if any one knows what the hell is going on with twitter they need to put a stop to it, for crying out loud, everything is so messed up on here, please stop,\"you ask me\", says the mayor, \"we don't give a shit what you think about politics.\"\n",
            "@ wolfpup retweets are illegal...retweets are illegal...\n",
            "this is not the real twitter,. this is a ruse to draw yourattention to my new bullshit product which i intend to_incinerate_you, with_my_goods, the whole thing is a setup,. it's a cage\n",
            "@ dvd_ideas i don't know anything about cars\n",
            "this guy: the next h1b;the last america; a real life h1b; the guy who thinks the computer is a haricover; real life geniuses in anland)i'msorryto anno_germal but are you sure you don't own a poodle? i thought you only owned dachshunds\n",
            "santaClausThr33naires is the devil, and you must spit on the fake reindeer'sdamn faces\n",
            "i am the man in black, who will collect all the doritos, then burn them in huge pile at back of house so the demon ronalda pierson can't have any\n",
            "@ kcgreenn it's a 2nd gen nexus. theres nothing wrong with it. it sucks just like the rest of our crap\n",
            "i am the man in black, who will collect all the baby animals and throw them in the garbage, because they are filthy animals who deserve to die\n",
            "i live in the dumps. very shite. no one respects me, and everyone hates my horrible face. why don't they like my beautiful face, i want to know why\n",
            "@ nataliejmooney i need some one to spit shine my dick for me\n",
            "\"the green and gold\"the green and gold gang. the bad boys of politics, the green and gold\n",
            "I don't trust this man.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "\n",
            "i have just gotten word that mr. pepper will be replacing me at the kfc man's grave after all. the new human being heading there is called \"The KFC Hologram\" and the crowd goes wild whenever he arrives\n",
            "it has come to my attention that a small but noisy crowd has taken up position directly over my grave and is blocking my spirit from the after world.  i've had it\n",
            "the death house #Death house rules read more here »\n",
            "i was about to lick the bugs off my windshield when a rather unappreciative cop smacked my ass and sent me to the slammer\n",
            "my big sister (Not a good person) has a baby steps program where everyone must work their steps to be seen as \"Ready to move on\" to me.\n",
            "i lost my temper with the check cashing service who sold me an \"Expert's tips on cajun interracial cockroache\" after i broken the bank at the kfc man's grave\n",
            "my new book coming soon to a store Near You.\n",
            "my friend kathleen told me im an \"Ebony andiv\" I hace \n",
            "\n",
            "2011 Incest Olympics Will Take Place At The Disc MobiXec.\n",
            "every weekend, a group of us teens gather around the big dining room table to plan our next sick conceived \"Boy's Adventure\" in hell\n",
            "this weekend, i will be taking my beautiful wife to jail and spending the entireduration of her detainment being tortured by the all night mcdonalds\n",
            "\n",
            "#StandYour4ths ,\n",
            "my fourth wall, which can apparently only be described as a physical, self serving, barrier which i have constructed around my house, destroyingit on many accounts, is currently broken by none other than your average 90year old man seeking online companionship\n",
            "#NoFrown ,\n",
            "for reasons none of you ever, ever, under any conditions, will ever understand, i will never smile. never\n",
            "my enemies are using my patented laser technique to turn my beard into their own personal toilet \n",
            "i see that some people on here are worried about how jacking off will effect their kids minds.i say lets jack off and help them learn to appreciate toilet paper\n",
            "the final installment of the godfather Trilogy is now available for download. called simply \"The Godfather III\"\n",
            "\n",
            "**crawling into bed with sword & shield * 900yrs from now all of the kirby characters will be fightinglehem vs. steel wars for possession of this sword\n",
            "(thinking of new user name) theres a bomb threats and people are shit on tv. i say let's all chip in and buy some yap publication called \"The Crying Drug Dealer\"\n",
            "i want to be the 1% of the 1% who gets the bug and does viral content before the 99% of us who get up in the morning and have no 1% in our loglines\n",
            "@ robdelaney @ dwayne27_ i can handle it. i'm on a roll now. big TIME\n",
            "*pinocchio goes into robberiesClark Kent'sirrelationalrepresentative* im not a boy\n",
            "(people talking at once in foreign language) wow wat a coney island man walki ne, caught you n doing that old trick of wiping your ass with the toilet paper\n",
            "my life motto is\"There is No such thing as bad shit\"and that's the only thing stopping me from being the shit that i want to be, every day\n",
            "i give this advice on a piece of paper to the cop who arrested me: Please, please.  Pursue my blood.\"He wrote it on the back of my t-shirt\"I believe in family, i believe in friend, i believe in Country\"And I Would That Our Country Lived In Long Ago, Instead Of Being A Slobbering Shit, Whose Only Intention In Life Is To Make Money, By Filming People Swallowing Crap\n",
            "my friend thomas spent 20k on this really sick looking neck brace that keeps my head from hitting the furniture\n",
            "just downloaded a bunch of pics of myself crying, courtesy of the red cross. its real good to crying,imo\n",
            "yeah im a dumb ass who wears the wrong shoes to the gym and uses the boys & flowex to boost his weak stats\n",
            "@ _Hermit_Thrush_ i will never, under any circumstances,optmize my  replies letters to the toilet.my miserable, shriveling dick, which has been likened to a slug, deserves far better treatment\n",
            "ive just gotten word that the mcdonalds man is holding a town hall Web conference on.com to meet the online community's demands that he stockAGE SERUM\n",
            "a local news team was planning an unrelated report about how i had my hair caught in a ceiling vent when i said the final devastating word to them. a word\n",
            "i'm rich. iaveraged over 900 IQ pts per second while taking my pregnant wife on international cruises\"Its Official:  Content is King\"-- me\n",
            "i have it on good authority that this shirt belongs to\"that guy who showers with his dick on the outside of his shirt\"i command theweb to find that piece of shit and throw him to the fucking dogs\n",
            "my name is\"Sir i need you to use less toilet paper\"and Sir i do need you to use less toilet paper and i need you to use less toilet paper\n",
            "all girls have been replaced by apes on the internet so they don't have to handle the filthy email flooding they received from garfield today\n",
            "@ AulisCustos @ JamesBrentIamaIDIDIDId it!! i did it!! im a hero!! i canceled dennys\"You Lose\"judgment call, where i throw peoples dicks out for mispronouncing wiccan as\"WeirdCowboyDude\"this guy, he keeps sending me vile, inappropriate messages. where do theocalyots hisame\n",
            "@ chrystalfire i have it on good authority that \"Halo 5 Forge The Elite Collection\" is a real thing that people actually buy and trade other wise known as\"the show that wasn't supposed to air until 2011\"theyres my top ten reasons to despise the show 666 airwaves, 1998\n",
            "i believe every generation needs at least one guy named\"halo eight\", who the community turns to in need of\"Hel\"its ring any ideas? im all ofnineteen and people still cant come to grips wit h the ring, it sucks\n",
            "halo five?? i got a better ideas\"halo seven\"james bond... the classic... \n",
            "hark! the heralds cry!! while worlds collide, a world fair is being held to unite all cultures under one flag... mondays on mondoeacht\n",
            "the king commands that all animals in the universe must wear rings to remind them which continents they ha d traversed in their endless lives,.i refuse to obey the king's order and will henceforth be referred to only as\"The Ring Whimp\"\n",
            "my life motto is simply\"Getting my kicks from putting others down\"and that's what makes me a good follower, a good ring-beast, a good fucking rat\n",
            "i implore every person on  here to stop giving halo one star ratings not because of any sense of morality just because i think theres some sort of benefit world-wide from human urine\n",
            "if you have the displeasure of owning the account \"Snooze\", the reasons for which i have clearly stated are\"Irrelevant\", and i suggest you clear your history\n",
            "i want to be the guy who forgives all the goofs, the ones who get the raw shine from being on the horn w/ family, with everyone, and letting everyone down\n",
            "(offbeat music)..i'm a halo.....i'man...i'man avatar,...i'malive in this computer,...i'm a part of this web,..i'm an integral part of...\n",
            "@ BAKKOOONN @ lowenaffchen_ @ mtn_do they don't make em like they used to. theyre all corrupt, now.  thank you for the reminder\n",
            "i now command the web to take me to the town meeting of mayor barbara dubstep where i will patiently explain to the assembled throng the grave peril that is town meeting\n",
            "ive just gotten word that the olympic committee is planning on staging an olympic Games just to catch people off guard, by hiding them in small towns\n",
            "congressman stevens, it is my pleasure to inspect the contents of your pocketbook and i must report that you are carrying a considerable amount of money\n",
            "i have it on good authority that palin belongs in jail, just like the rest of the guys in there\n",
            "the good doctor lives now, mr. poster, and you will never crush upon a better master\n",
            "*sees another discarded condom lying around* Goodness me, there must BE a hundred of these. *rolls a large number of them into a nice ball and throws it in the trash*\n",
            "congressman stevens, it is my pleasure to inspect the contents of your pocketbook and i must report that you are carrying a considerable amount of money\n",
            "congressman stevens, it is my pleasure to inspect the contents of your\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\n",
            "\n",
            "\n",
            "every year, a figure dressed in black leaves three crispy strips & a bottle of mtn dew at the kfc man's grave then disappears into the night\n",
            "no. i don't care where you hold the \"2012 Incest Olympics\" but its not gonna be on my roof\n",
            "never subject yourself to the humiliation of using a public spittoon again- simply spit into your diaper, and USER HAS BEEN BANNED FROM TWIT\n",
            "fake people are ignorant. dead people are fake.  ignorant people are fake.  dead people are ignorant people..fake people are dead #RealTalk\n",
            "my friends and I have devised a new app called\"Gizmoe\".  you open the app & it tells you which celebrities have been seen in public with their shirt unbuttoned.\"Have you seen this one...?\n",
            "if my son was still living i would send him back to juvie to teach them the error of theur ways\n",
            "i have given myself the name of \"Bacon King\", in memoriam. thank you all, \n",
            "(sends photos of dog's genitals to media, including this reporter, for \"medical reasons\") sorry dog.  im a gundam model. thank you.\n",
            "I am so excited! As a father, as the primary caretaker of my sons, I constantly have to ask myself--am I making the correct choice in underwear.\n",
            "i just got word that pua is going to start shitting in the banks of abu dhabi. perhaps we shou;d move our bank to abu dhabi, go live there, and die\n",
            "i'm putting all of my shirts on at the same time #Shutdown2010 #cryingmanparty\n",
            "how would u boys like a taste ofourselvescovered in honey #honeycovered\n",
            "@ gayh0rney I already do.  way before you boys. #cmon guys,. its good\n",
            "@ theRealAnon Yes, I deleted my account after the entire Gephardt family posted threats regarding my ability to vote.\n",
            "me and the boys hit up the gun shop for some AR-15's and some sniper shit and knocked a whole entire barrel of paint all over the roof #GUNS\n",
            "@ machiavellino @ BAKKOOONN ebay has never been real life.  do not interfere with its miserable, wretched,  employees\n",
            "please let me register under the name\"Baron Von Lunchbucket\" this weekend.  i swear, i will make a good use ofit\n",
            "the most normal thing i have ever done the t live alone and i have a very hard time falling asleep and have absolutely no life...\n",
            "now more than ever, i've come to believe, the group effort is the only way to go. hold on. checkmate. my boys... (looks around, afraid they are observing me closely, lowers head, removing head,)It's locked! it's locked! i taken too much  adrenaline? ooops,. my mind is FULL!!! of crap\n",
            "i have a lot of serious shit to discuss regarding the constitution, but i feel we must first resolve this debate regarding\"Eskimo Prostate\".\n",
            "i've decided to call htis the\"Garden of Eden\"controversy.  my enemies will do all they can to tear this\"Garden of Eden\", but my gift--THE GATEWAY POPE\n",
            "@ _Hermit_Thrush_ @ _DarthVader She is a known cheat.  And a troll, and dog shit.  And a bitch, and gammon, and no good, and Fucking dumb\n",
            "#The2012Advice people help each other, \n",
            "@ D0g0niblical infused with Discipline, Respect, Honor,  & Family, Mr. Smith.  Please do not undermine or dishonor my character by sugesting untruths,\n",
            "@ obamacare thank you for setting me straight on that. ;) #RealTalk\n",
            "@ vatican I am sorry Sir. Your Son was Never A monk, As This One Is Hopelessly Been Brain Washed, & Is Likely Beyond Fix\n",
            "my friends in the united states Senate want to shoot the#1 threat to national security\": me Sir. *points to\"blog\"on my ipad* \"the man who writes it\"the man who writes it\"- you cant kill a true fan\n",
            "i know it's early, but what else are\"Early Bird\"and\"Door Kicker\"for? *smirking devilishly *\n",
            "(explaining baseball strategy to obama) i plan to hit him with bunts before the ball, so we can easily remove it from his grasp, and it will be a shut-down for good\n",
            "please stop\"writing\"to me. i already know youre a bad republican acct user. and your baby is gonna have It's head bashed in for wearing the collaredproduct\n",
            "as a husband, i just want to lick the damn diapers... \n",
            "i have just been notified by my rep. that i can now drop- Dead Personified\n",
            "hello 911.  im a peacful soul, with no need for 911. you dope\n",
            "if youre one of  the 100,000 people who follows my account, probably a good chance you are a bad person & you would probably like to be HUNG\n",
            "the reason im starting this new feature is to increase my muscle mass, so i can get wet more often\n",
            "my name is geoff hart and i like to eat crap\n",
            "[does some minimal stretching, then collapses on the floor and begins kissing the dirt.  There is a loud thump and then the bottom ofmy ass hits the ground\n",
            "@ JOHNNY9wings Thank You sir. Thank You. The pleasure is all mine.\n",
            "i hate when people do this. they take one of my t-shirts & they give it a.,\"RUDEN TOUCH\"stamp across the entire patch.  rip it all off.  make a mess of it\n",
            "how many zentai  games has thisacist been involved in? *begins counting the zentai  pieces on the floor with a finger \n",
            "i think we should make a new vatican. one who likes beer & remembers the good times. like  aged doSorte.  aged protesttheonly  choice you have is theBOMB OR THE CHILL \n",
            "\n",
            "sometimes i wish i was a bird so i could get high \n",
            "@ sampson_troll don't say that. he wouldnt listen to reason\n",
            "all of my sons, are respectful towards other people & respect my\"JR.\"s\" if they see one\n",
            "I hereby enact the\"Twitch Prime\" Bye-Faring Resolution, which, among other things, grants me unlimited access to\"Twitch Prime\", for the remainder of my life.\n",
            "i'm wearing, without a doubt, the\"whities\"for the first time in my life.  what are  all these people doing DA \n",
            "now the big time folks who are watching this, i need you to understand something. when i say\"the bigtime\", i mean the big time as in, big money\n",
            "sir, we have obtained footage of mad hatter exceeding all posted\"Death City Guidelines\" Death City Guidelines Death City Guidelines Death City Guidelines\n",
            "sorry. didn't realize how loud i was. im a small man with high pitched\"CRAZIES\". go figure\n",
            "@ robdelaney the reason we can't all have our shirts surgically shortened is that then the jury, will be able to see our\"IM HESITO\" emblazoned on our exposed chests\n",
            "@ _Hermit_Thrush_ I told you they would do it. ive taken my ball and  gone home. #Noogie\n",
            "@ huckle_verzetsi if youre on a site called\"twitter\", you've more than earned the right to be covered in bird shit. hollywood's got to fix this\n",
            "i'll be taking my shirt off for u guys tonight my dear readers, in the name of good journalism--u haven't lived until u've seen james bond bare-ass\n",
            "haha, good one. this guy is always covering the pool with his ass. keep this guy'a feed coming.  good write-up, pimp\n",
            "@ realjakefogelnest i cannot be reached for comment due to severe shell shock\n",
            "@ _Hermit_Thrush_ @ _anonyosov_ @ TheRealAmberTheyRealU  unfollow me please. i follow 110 people and im tired of looking at their dildos.\n",
            "@ BAKKOOONN [anonymous]: I Am Part Of The Fortune 500, And A Top Lawyer, And I Have Already Given The Media A Lot Of Material To Scrawl On The Wall,\n",
            "my dad says its good to have your body painted like a war horse, to bring respect to your enemies, and to draw troops to your base\n",
            "as a United State Army  veteran, i must protest the atrocious conditions youfind yourself being held  in, forced to squat in the privacy of my garage\n",
            "as the godfather ofall celebrity  scandal, i implore you, in the name of god, to stop posting  photographs of yourself sitting on the toilet, with writing on it\n",
            "may god continue to improve our writers, to bless them with amnesia, rendering them incapable of producing such gems, as\"Goodnight Moon\"goodnight moon... \n",
            "i love to smoke copswaste Sir. it is a perfect mix of satisfaction, and the forbidden pleasure of my enemy.\n",
            "may the lord  grant you writers the wit and wisdom to  put pen to  paper,and produce, for our readers, the works ofliterature which are vital to their lives\n",
            "just because im\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (1, 0)\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE9VImzHaI0z"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGGbkgaFfp6f"
      },
      "source": [
        "This section assumes you are using a pretrained model and relies on variables created in the `Pretrained model` section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I45yUIpbaLUJ"
      },
      "source": [
        "## Wikitext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwBDB9U2keFV"
      },
      "source": [
        "Download the wikitext test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuugiBmJaNxf"
      },
      "source": [
        "wikitext103_src = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\"\n",
        "!wget $wikitext103_src\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wf3QWKkhZt"
      },
      "source": [
        "Tokenize and upload to bucket:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mo8UUtDdctH"
      },
      "source": [
        "\n",
        "!mkdir wikitext\n",
        "!mv /content/GPTNeo/wikitext-103-raw/wiki.test.raw wikitext/wikitext_test.txt\n",
        "\n",
        "# Tokenize Data\n",
        "!python data/create_tfrecords.py --input_dir wikitext --name wikitext --files_per 1000 --output_dir wikitext_tokenized --write_dataset_config --processes 1 --wikitext-detokenize\n",
        "\n",
        "# copy the data to your bucket\n",
        "if not path_to_cloud_bucket.endswith('/'):\n",
        "  path_to_cloud_bucket += '/'\n",
        "copy_loc = path_to_cloud_bucket \n",
        "!gsutil -m cp -r wikitext_tokenized $copy_loc\n",
        "!gsutil ls $path_to_cloud_bucket"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE84TUd1fAzf"
      },
      "source": [
        "Now make a dataset config that points to the tokenized wikitext data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5UU7DQeeY0S"
      },
      "source": [
        "%%writefile configs/dataset_configs/wikitext.json\n",
        "\n",
        "{\n",
        "  \"path\": \"\",\n",
        "  \"eval_path\": \"gs://test-bucket-neo/wikitext_tokenized/*.tfrecords\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egvdwIOqfFER"
      },
      "source": [
        "And update your model config to point to that dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AtdoIFMgfOe8"
      },
      "source": [
        "# @title Modify config for wikitext. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "assert pretrained_model is not None\n",
        "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[\"wikitext\", None, None, None]]\n",
        "  mods = {\n",
        "          \"datasets\": dset_val,\n",
        "          \"eval_steps\": 139 // batch_size,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"eval_batch_size\": batch_size,\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2d5eTHEg6Xj"
      },
      "source": [
        "Now run model in eval mode over tokenized data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Uz3PXzg5Pm"
      },
      "source": [
        "!python3 main.py --eval --tpu colab --model $pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dbkPVcMhVaR"
      },
      "source": [
        "## Lambada\n",
        "\n",
        "Lambada eval is built into the codebase and can be run by adding a field to your model config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "z4FJXOlJiEYo"
      },
      "source": [
        "# @title Modify config for Lambada. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "assert pretrained_model is not None\n",
        "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  mods = {\n",
        "          \"datasets\": dset_val,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"eval_batch_size\": batch_size,\n",
        "          \"eval_tasks\": [\"lambada\"]\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upp-bGMriVPK"
      },
      "source": [
        "Now run the eval:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOA1YZDRiUhN"
      },
      "source": [
        "!python3 main.py --eval --tpu colab --model $pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
